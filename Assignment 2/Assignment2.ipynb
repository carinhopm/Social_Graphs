{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, operator, string, math\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk import FreqDist\n",
    "from fa2 import ForceAtlas2\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import networkx as nx\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "#import preprocessor as p\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk, string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import os\n",
    "\n",
    "sns.set(style='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Twitter Network Analysis\n",
    "\n",
    "**_Exercise_ 1: Build the network of retweets.\n",
    "We will now build a network that has as nodes the Twitter handles of the members of the house, and direct edges between nodes A and B if A has retweeted content posted by B. We will build a weighted network, where the weight of an edge is equal to the number of retweets. You can build the network following the steps below (and you should  be able to reuse many of the functions you have written as part of the exercises during the previous weeks):**\n",
    "\n",
    "* **Consider the 200 most recent tweets written by each member of the house (use the files [here](https://github.com/suneman/socialgraphs2019/tree/master/files/data_twitter/tweets_2019.zip/). For each file, use a regular expression to find retweets and to extract the Twitter handle of the user whose content was retweeted. All retweets begin with \"*RT @originalAuthor:*\", where \"*originalAuthor*\" is the handle of the user whose content was retweeted (and the part of the text you want to extract).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [file](https://github.com/suneman/socialgraphs2019/tree/master/files/data_twitter/tweets_2019.zip/) has been downloaded and unzipped in the current path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing variables\n",
    "all_rts = {}\n",
    "folder = \"tweets_2019_full/\"\n",
    "\n",
    "# Read each file\n",
    "for filename in os.listdir(path=folder):\n",
    "    all_rts[filename] = []\n",
    "    filepath = \"{}{}\".format(folder, filename)\n",
    "    f = open(filepath,\"r+\",encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    # Find all retweets within the current file\n",
    "    for line in lines:\n",
    "        retweets = re.findall(r\"RT @\\w+\", line)\n",
    "        counter = 0\n",
    "        # Save the first 200 retweets\n",
    "        for retweet in retweets:\n",
    "            if counter<200:\n",
    "                all_rts[filename].append(retweet[4:])\n",
    "                counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderPath = \"tweets_2019_full/\"\n",
    "tweets = {}\n",
    "\n",
    "for file in sorted(os.listdir(folderPath)):\n",
    "    filepath = os.path.join(folderPath, file)\n",
    "    f = open(filepath, 'r', encoding=\"utf8\")\n",
    "    tweets[file] = f.read()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **For each retweet, check if the handle retweeted is, in fact, the handle of a member of the house. If yes, keep it. If no, discard it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the [file](https://github.com/suneman/socialgraphs2019/blob/master/files/data_twitter/H115_tw_2019.csv) containing the data of the 115th Congress has been downloaded in the current path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data of all the politicians\n",
    "df = pd.read_csv('H115_tw_2019.csv')\n",
    "twitterHandle = df\n",
    "df = df.set_index('tw_name') # setting 'tw_name' column as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_names = []\n",
    "for i in range(len(twitterHandle)):\n",
    "    \n",
    "    #name = twitterHandle[i]['tw_name']\n",
    "    tw_names.append(twitterHandle.loc[i]['tw_name'])\n",
    "    \n",
    "\n",
    "##Filtering out duplicates\n",
    "tw_names =  set(tw_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing variable\n",
    "pol_rts = {}\n",
    "\n",
    "# Filter retweets\n",
    "for key, values in all_rts.items():\n",
    "    pol_rts[key] = []\n",
    "    for value in values:\n",
    "        # Save retweet only if belongs to members of the house\n",
    "        if value in df.index:\n",
    "            real_name = df.loc[value]['WikiPageName']\n",
    "            if key != real_name: # avoiding self-loops\n",
    "                pol_rts[key].append(real_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Use a NetworkX [`DiGraph`](https://networkx.github.io/documentation/development/reference/classes.digraph.html) to store the network. Use weighted edges to account for multiple retweets. Also store the party of each member as a node attribute (use the data in [this file](https://github.com/suneman/socialgraphs2019/blob/master/files/data_twitter/H115_tw_2019.csv). Remove self-loops (edges that connect a node with itself).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tw_name'] = df.index\n",
    "df = df.set_index('WikiPageName') # setting 'WikiPageName' column as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes\n",
    "for index, row in df.iterrows():\n",
    "    G.add_node(index, party=row[0])\n",
    "    \n",
    "# Add edges\n",
    "for pol, links in pol_rts.items():\n",
    "    links_counter = Counter(links)\n",
    "    for link, count in links_counter.items():\n",
    "        G.add_edge(pol, link, weight=count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **_Exercise_ 2: Visualize the network of retweets and investigate differences between the parties.**\n",
    " * **Visualize the network using the [Networkx draw function](https://networkx.github.io/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw.html#networkx.drawing.nx_pylab.draw), and nodes coordinates from the force atlas algorithm. *Hint: use an undirected version of the graph to find the nodes positions for better results, but stick to the directed version for all measurements.* Plot nodes in colors according to their party (e.g. 'red' for republicans and 'blue' for democrats) and set the node-size to be proportional to total degree.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'forceatlas2' configuration\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=False,  # Dissuade hubs\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=1.5,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=1.0,  # Tolerance\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=1.2,\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=0.0001,\n",
    "                        strongGravityMode=False,\n",
    "                        gravity=0.5,\n",
    "\n",
    "                        # Log\n",
    "                        verbose=True)\n",
    "\n",
    "# Create undirected graph\n",
    "Gu = G.to_undirected()\n",
    "\n",
    "# Calculate positions\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(Gu, pos=None, iterations=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map parties to colors\n",
    "parties_map = {'Republican':'Red',\n",
    "              'Democratic':'Blue'}\n",
    "\n",
    "# Plot graph\n",
    "plt.figure(figsize=[15,15])\n",
    "nx.draw_networkx_nodes(Gu, positions, with_labels=False, alpha=0.7, \n",
    "                       node_size=[5*s for s in dict(Gu.degree(Gu.nodes())).values()],\n",
    "                       node_color=[c for c in [parties_map.get(Gu.node[n]['party']) for n in Gu.nodes()]])\n",
    "nx.draw_networkx_edges(Gu, positions, edge_color=\"black\", alpha=0.2, arrows=False)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Compare the network of Retweets with the network of Wikipedia pages (generated during Week 5). Do you observe any differences? How do you explain them?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, there are some differences.\n",
    "\n",
    "The Wikipedia-graph has interactions between nodes of different parties, resulting in a very mixed graph, but the Retweet-graph has only few interactions between members of different parties. As a result, the Retweet-graph has two different zones, one per party, with 2-3 isolated members of the other party. Both zones are connected by some edges just in the center of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Now set the node-size tob proportional to betweenness centrality. Do you observe any changes?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate betweenness centrality\n",
    "btwss = nx.betweenness_centrality(G)\n",
    "\n",
    "# Plot graph\n",
    "plt.figure(figsize=[15,15])\n",
    "nx.draw_networkx_nodes(Gu, positions, with_labels=False, alpha=0.7, \n",
    "                       node_size=[10000*btwss[n] for n in Gu.nodes()],\n",
    "                       node_color=[c for c in [parties_map.get(Gu.node[n]['party']) for n in Gu.nodes()]])\n",
    "nx.draw_networkx_edges(Gu, positions, edge_color=\"black\", alpha=0.2, arrows=False)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The members that have links with members of the other party increase considerably in size compared to those that do not. The size of a node is proportional to the number of shortest paths between all node pairs that run along that node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Repeat the point above using eigenvector centrality. Again, do you see a difference? Can you explain why based on what eigenvector centrality measures?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate eigenvector centrality\n",
    "eigen = nx.eigenvector_centrality(G)\n",
    "\n",
    "# Plot graph\n",
    "plt.figure(figsize=[15,15])\n",
    "nx.draw_networkx_nodes(Gu, positions, with_labels=False, alpha=0.7, \n",
    "                       node_size=[2000*eigen[n] for n in Gu.nodes()],\n",
    "                       node_color=[c for c in [parties_map.get(Gu.node[n]['party']) for n in Gu.nodes()]])\n",
    "nx.draw_networkx_edges(Gu, positions, edge_color=\"black\", alpha=0.2, arrows=False)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes located in the republican side of the graph are bigger than the ones in the democratic side.\n",
    "\n",
    "Using the eigenvector centrality the nodes with connections to high-scoring nodes will have a higher score. It's easy to check that the republicans have more interactions between them taking a look to the list of nodes with a higher degree (check the next section of this exercise). As only a few links connect both zones, the average score in the republican zone remains high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Who are the three nodes with highest degree within each party? And what is their eigenvector centrality? And their betweenness centrality?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the degree of each node and order the results\n",
    "degree = dict(G.degree(G.nodes()))\n",
    "degree_rank = dict(sorted(degree.items(), key=operator.itemgetter(1), reverse=True))\n",
    "\n",
    "# Print values for top3 nodes of each party\n",
    "print(\"The 3 politicians of each party with higher degree are:\")\n",
    "dem_count = 0\n",
    "rep_count = 0\n",
    "for key, value in degree_rank.items():\n",
    "    if G.node[key]['party']=='Democratic' and dem_count<3:\n",
    "        print(\"The democrat \"+df.loc[key]['Name']+\" with degree \"+str(value)\n",
    "              +\", eigenvector centrality \"+str(round(eigen[key],ndigits=3))\n",
    "              +\" and betweenness centrality \"+str(round(btwss[key],ndigits=3)))\n",
    "        dem_count += 1\n",
    "    elif G.node[key]['party']=='Republican' and rep_count<3:\n",
    "        print(\"The republican \"+df.loc[key]['Name']+\" with degree \"+str(value)\n",
    "              +\", eigenvector centrality \"+str(round(eigen[key],ndigits=3))\n",
    "              +\" and betweenness centrality \"+str(round(btwss[key],ndigits=3)))\n",
    "        rep_count += 1\n",
    "    elif dem_count==3 and rep_count==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Plot (on the same figure) the distribution of outgoing strength for the republican and democratic nodes repectively (i.e. the sum of the weight on outgoing links). Which party is more active in retweeting other members of the house?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing variables\n",
    "N = len(list(G.nodes))\n",
    "dem_out_w = []\n",
    "rep_out_w = []\n",
    "\n",
    "# Calculating the outgoing strength\n",
    "for node in G.nodes:\n",
    "    if G.node[node]['party']=='Democratic':\n",
    "        dem_out_w.append(sum(G[u][v]['weight'] for (u,v) in G.out_edges([node])))\n",
    "    else:\n",
    "        rep_out_w.append(sum(G[u][v]['weight'] for (u,v) in G.out_edges([node])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions\n",
    "fig=plt.figure(figsize=[15,5])\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.hist(dem_out_w, bins=N//10)\n",
    "plt.title('Democratic outgoing strength')\n",
    "plt.xlabel('sum(out_edges_weight)')\n",
    "plt.ylabel('N(sum(out_edges_weight))')\n",
    "plt.xlim(-3,105)\n",
    "plt.ylim(0,70)\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.hist(rep_out_w, bins=N//10, color='Red')\n",
    "plt.title('Republican outgoing strength')\n",
    "plt.xlabel('sum(out_edges_weight)')\n",
    "plt.ylabel('N(sum(out_edges_weight))')\n",
    "plt.xlim(-3,105)\n",
    "plt.ylim(0,70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The members of the Republican party make more retweets than the democrats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Find the 3 members of the republican party that have retweeted tweets from democratic members most often. Repeat the measure for the democratic members. Can you explain your results by looking at the Wikipedia pages of these members of the house?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing variables\n",
    "dem_in_d = {}\n",
    "rep_in_d = {}\n",
    "\n",
    "# Calcule most retweeted politicians\n",
    "for node in G.nodes:\n",
    "    counter = 0\n",
    "    if G.node[node]['party']=='Democratic':\n",
    "        for i in G.in_edges([node]):\n",
    "            if G.node[i[0]]['party']=='Republican':\n",
    "                counter += 1\n",
    "        dem_in_d[node] = counter\n",
    "    else:\n",
    "        for i in G.in_edges([node]):\n",
    "            if G.node[i[0]]['party']=='Democratic':\n",
    "                counter += 1\n",
    "        rep_in_d[node] = counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the results\n",
    "dem_in_d_rank = dict(sorted(dem_in_d.items(), key=operator.itemgetter(1), reverse=True)[:3])\n",
    "rep_in_d_rank = dict(sorted(rep_in_d.items(), key=operator.itemgetter(1), reverse=True)[:3])\n",
    "\n",
    "# Print the results\n",
    "print(\"The 3 most retweeted democratic politicians by the republicans are:\")\n",
    "for key, value in dem_in_d_rank.items():\n",
    "    print(df.loc[key]['Name']+\" with \"+str(value)+\" retweets\")\n",
    "print(\"\\nThe 3 most retweeted republican politicians by the democrats are:\")\n",
    "for key, value in rep_in_d_rank.items():\n",
    "    print(df.loc[key]['Name']+\" with \"+str(value)+\" retweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On one hand, some facts that could explain why some republicans retweeted these democrats are:\n",
    "\n",
    "- Gene Green, despite the Democratic leadership's general disapproval of the Iraq war, voted against measures aimed at placing a timetable on military withdrawal.\n",
    "- Ted Deutch spoke out in favor of expanded gun control legislation after the Stoneman Douglas High School shooting (where 17 passed away).\n",
    "- Business groups have praised Derek Kilmer for being one of the most pro-business Democrats in Olympia.\n",
    "\n",
    "On the other hand, some facts that could explain why some democrats retweeted these republicans are:\n",
    "\n",
    "- Justin Amash has described himself as a libertarian, dissenting from both Republican and Democratic leaderships more frequently than the vast majority of Republican members of Congress. On July 4, 2019, he announced that he was leaving the Republican Party to become an independent.\n",
    "- Adam Kinzinger was ranked as the 40th most bipartisan (by measuring the frequency each member's bills attract co-sponsors from the opposite party and each member's co-sponsorship of bills by members of the opposite party) member of the U.S. House of Representatives during the 114th United States Congress.\n",
    "- David Schweikert supports allowing veterans access to medical marijuana, if legal in their state, per their Veterans Health Administration doctor's recommendation and voted twice in support of this in the Veterans Equal Access Amendment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Community detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Use your favorite method of community detection to find communities in the full house of representatives network. Report the value of modularity found by the algorithm. Is it higher or lower than what you found for the Wikipedia network (Week 7)? Comment on your result.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the louvain community detection algorithm to detect communities in the full house representatives network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "partition = community.best_partition(Gu)\n",
    "#calculate the number of communities\n",
    "nb_communities = len(set(partition.values()))\n",
    "\n",
    "# add a new \"community attribute\" to the nodes of the undirected graph\n",
    "nx.set_node_attributes(Gu, name='community', values=partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Calculated number of communities: {nb_communities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to store the members of each community as a list\n",
    "communities = {}\n",
    "for group in set(partition.values()): #<-- partition is a dictionary with names \n",
    "                                      #      as keys and communities as values\n",
    "    members = [key for key, value in partition.items() if value==group]\n",
    "    communities[group] = members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_partition = df.reset_index()[['WikiPageName', 'Party']].copy()\n",
    "# map party to 1 or zero repsectively\n",
    "party_partition.Party = party_partition.Party.apply(lambda x: 0 if x=='Republican' else 1)\n",
    "# create a dictionary same as community dictionary\n",
    "party_partition = party_partition.set_index(\"WikiPageName\").to_dict()['Party']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Modularity of Louvain Partition: {community.modularity(partition, Gu)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Modularity Based on Parties: {community.modularity(party_partition, Gu)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modularity found by the algorithm is 0.485 which is substantially larger than the one computed at week 7 ( around 0.15 ),  as well as the modularity based on party membership.\n",
    "\n",
    "This tells us that this partitioning describes better the network structure of the house of representatives. \n",
    "Also we can say that the data mined from twitter gave us a lot more information about the network structure as well as about the conections between person and between parties, than the wikipedia data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Visualize the network, using the Force Atlas algorithm. This time assign each node a different color based on their *community*. Describe the structure you observe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set our color palette\n",
    "palette = sns.color_palette(\"muted\", n_colors=nb_communities)\n",
    "\n",
    "# select color based on community\n",
    "node_color  = []\n",
    "for node in Gu.nodes(data=True):\n",
    "    node_color.append(palette[int(node[1]['community'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "nx.draw_networkx(Gu, positions, node_size=[100*v  for v in dict(nx.degree(Gu)).values()], with_labels=False, width=0.2, node_color=node_color, alpha=0.7)\n",
    "plt.axis('Off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Compare the communities found by your algorithm with the parties by creating a matrix $\\mathbf{D}$ with dimension $(B \\times C$, where $B$ is the number of parties and $C$ is the number of communities. We set entry $D(i,j)$ to be the number of nodes that party $i$ has in common with community $j$. The matrix $\\mathbf{D}$ is what we call a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = np.zeros((2, nb_communities))\n",
    "for node in Gu.nodes(data=True):\n",
    "    confusion_matrix[party_partition[node[0]], node[1]['community']] += 1\n",
    "    \n",
    "confusion_matrix = pd.DataFrame(confusion_matrix, \n",
    "                                index=['Republican', 'Democratic'],\n",
    "                                columns=range(nb_communities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also normalise the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix/len(list(Gu.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrix\n",
    "sns.heatmap(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Use the confusion matrix to explain how well the communities you've detected correspond to the parties. Consider the following questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- group 0 has the bulk of the democrats and group 2 most of the republicans\n",
    "- republicans are in general spread into more groups\n",
    "- there are many (4) groups that republicans and democrats coexist in the same numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Does the community detection algorithm sub-divide the parties? Do you know anything about American politics that could explain such sub-divisions? Answer in your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The community detection algorithm seems to detect sub-divisions of the parties. This could be senators from same or nearby states or for specific groups inside parties we really well defined jobs e.g ethics committee. Another group detected by the algorithm is that of the independant senators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: What do republican and democratic members tweet about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Exercise_ 4: TF-IDF of the republican and democratic tweets.**\n",
    "\n",
    "**We will create two documents, one containing the words extracted from tweets of republican members, and the other for Democratic members. We will then use TF-IDF to compare the content of these two documents and create a word-cloud. The procedure you should use is exactly the same you used in exercise 2 of week 7. The main steps are summarized below:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Create two large documents, one for the democratic and one for the republican party. Tokenize the pages, and combine the tokens into one long list including all the pages of the members of the same party.**\n",
    "  * **Exclude the twitter handles of other members.**\n",
    "  * **Exclude punctuation.**\n",
    "  * **Exclude stop words (if you don't know what stop words are, go back and read NLPP1e again).**\n",
    "  * **Exclude numbers (since they're difficult to interpret in the word cloud).**\n",
    "  * **Set everything to lower case.**\n",
    "  * **Compute the TF-IDF for each document.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing variables\n",
    "folder = \"tweets_2019_full/\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('amp') # in order to avoid '&amp' symbols\n",
    "\n",
    "# Create files\n",
    "dem_filepath = \"democrats.txt\"\n",
    "rep_filepath = \"republicans.txt\"\n",
    "dem_file = open(dem_filepath,\"w+\",encoding='utf-8')\n",
    "rep_file = open(rep_filepath,\"w+\",encoding='utf-8')\n",
    "dem_file.close()\n",
    "rep_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean & tokenize the words within a tweet\n",
    "def clean_tweet(tweet):\n",
    "\n",
    "    clean_tweet = re.sub(r'@\\w+', '', tweet) # Remove mentions\n",
    "    clean_tweet = re.sub(r'#\\w+', '', clean_tweet) # Remove hashtags\n",
    "    clean_tweet = re.sub(r'http[s]?:\\/\\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
    "                         '', clean_tweet) # Remove URL's\n",
    "    clean_tweet = re.sub(r'\\d+(.\\d+)?', '', clean_tweet) # Remove numbers\n",
    "    clean_tweet = clean_tweet.translate(str.maketrans('', '', string.punctuation)) # Remove punctuation symbols\n",
    "    clean_tweet = re.sub(r'…', '', clean_tweet) # Remove consecutive dots (special character)\n",
    "    clean_tweet = re.sub(r'[\\“\\”\\’]', '', clean_tweet) # Remove special quotes and double quotes\n",
    "    clean_tweet = re.sub(r' +', ' ', clean_tweet) # Remove unnecesary whitespaces\n",
    "    clean_tweet = ' '.join( [w for w in clean_tweet.split() if len(w)>1] ) # Remove isolated letters\n",
    "    tweet_tokens = word_tokenize(clean_tweet.lower())\n",
    "    filtered_tokens = []\n",
    "    for w in tweet_tokens: \n",
    "        if w not in stop_words: # Filter stop words\n",
    "            filtered_tokens.append(w) \n",
    "    clean_tweet = ' '.join( [w for w in filtered_tokens] )\n",
    "    \n",
    "    return clean_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "df = pd.read_csv(\"H115_tw_2019.csv\")\n",
    "df = df.set_index(\"WikiPageName\")\n",
    "\n",
    "# Read the files of the members of the house\n",
    "for filename in os.listdir(path=folder):\n",
    "    filepath = \"{}{}\".format(folder, filename)\n",
    "    f_read = open(filepath,\"r+\",encoding='utf-8')\n",
    "    lines = f_read.readlines()\n",
    "    f_read.close()\n",
    "    current_filepath = ''\n",
    "    # Check the member's party\n",
    "    if df.loc[filename]['Party']=='Democratic':\n",
    "        current_filepath = dem_filepath\n",
    "    else:\n",
    "        current_filepath = rep_filepath\n",
    "    f = open(current_filepath,\"a+\",encoding='utf-8')\n",
    "    \n",
    "    # Clean the tweets\n",
    "    for line in lines:\n",
    "        retweet = re.findall(r\"RT @\\w+\", line) # Discard retweets\n",
    "        if not retweet:\n",
    "            filtered_tweet = clean_tweet(line)\n",
    "            f.write(filtered_tweet)        \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files\n",
    "dem_file = open(dem_filepath,\"r+\",encoding='utf-8')\n",
    "rep_file = open(rep_filepath,\"r+\",encoding='utf-8')\n",
    "dem_words = dem_file.read()\n",
    "rep_words = rep_file.read()\n",
    "dem_file.close()\n",
    "rep_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TF-IDF\n",
    "def calc_tf_idf(tokens,doc_token_sets):\n",
    "    \n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    ndocs = len(doc_token_sets)\n",
    "    tf_idf_list = []\n",
    "    for item in fdist.most_common():\n",
    "        word = item[0]\n",
    "        cnt = 0\n",
    "        for i in range(ndocs):\n",
    "            if word in doc_token_sets[i]:\n",
    "                cnt += 1  \n",
    "        try:\n",
    "            idf = math.log(ndocs/cnt)\n",
    "            tf = item[1]\n",
    "\n",
    "            tf_idf = tf*idf\n",
    "            tf_idf_list.append((word,tf_idf))\n",
    "        except:\n",
    "            print('Exception')\n",
    "    tf_idf_list = sorted(tf_idf_list, key=lambda x: x[1],reverse = True)  \n",
    "    \n",
    "    return tf_idf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "# Tokenize\n",
    "dem_tokens = word_tokenize(dem_words)\n",
    "rep_tokens = word_tokenize(rep_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Please be pacient, the next cell takes a long time to compute...</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF\n",
    "dem_tfidf = calc_tf_idf(dem_tokens, list(set(dem_tokens)))\n",
    "rep_tfidf = calc_tf_idf(rep_tokens, list(set(rep_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order words by TF-IDF\n",
    "sorted_dem_tfidf = sorted(dem_tfidf, key=lambda kv: kv[1], reverse=True)\n",
    "sorted_rep_tfidf = sorted(rep_tfidf, key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top10 results for each party\n",
    "print(\"Democrats top10 words:\")\n",
    "print(str(sorted_dem_tfidf[:10])+\"\\n\")\n",
    "print(\"Republicans top10 words:\")\n",
    "print(sorted_rep_tfidf[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Now, create word-cloud for each party. Are these topics less \"boring\" than the Wikipedia topics from Week 7? Why?  Comment on the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blue_color_func(word, font_size, position,orientation,random_state=None, **kwargs):\n",
    "    return(\"hsl(230,100%%, %d%%)\" % np.random.randint(49,51))\n",
    "def red_color_func(word, font_size, position,orientation,random_state=None, **kwargs):\n",
    "    return(\"hsl(355,100%%, %d%%)\" % np.random.randint(49,51))\n",
    "\n",
    "dem_wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                      background_color='white',\n",
    "                     ).generate(dem_words)\n",
    "dem_wordcloud.recolor(color_func = blue_color_func)\n",
    "rep_wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                      background_color='white'\n",
    "                     ).generate(rep_words)\n",
    "rep_wordcloud.recolor(color_func = red_color_func)\n",
    "\n",
    "fig=plt.figure(figsize=[15,10])\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow(dem_wordcloud)\n",
    "plt.axis('off')\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(rep_wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics are more interesting because the nature of the text. The tweets are limited to 280 characters, so the politicians have to express their ideas and slogans briefly to fit them. In the case of the Wikipedia pages the texts are not written by politicians and contains some general information about each member of the house, so the word-cloud contains less relevant words to characterize each party."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Sentiment over the Twitter data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Download the LabMT wordlist. It's available as supplementary material from Temporal Patterns of Happiness and Information in a Global Social Network: Hedonometrics and Twitter (Data Set S1). Describe briefly how the list was generated.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LabMT wordlist contains a happiness ratings of over 10.000 english words. The wordlist is downloaded from the following link: [Data](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026752#pone.0026752.s001). The list was generated using the Amazon's Mechanical Turk Service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = 'LabMT_wordlist.txt'\n",
    "\n",
    "labmt = pd.read_csv(filePath, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating a dictionary to store the words from the lamMT for a easy look-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_happiness={}\n",
    "for word in labmt['word']:\n",
    "    word_idx = labmt[labmt['word']==word].index[0]\n",
    "    word_happiness[word] = labmt.loc[word_idx,'happiness_average']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Based on the LabMT word list, write a function that calculates sentiment given a list of tokens (the tokens should be lower case, etc).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sentiment(tokens,word_happiness):\n",
    "    \n",
    "    happiness_avg = [word_happiness[t.lower()] for t in tokens if t.lower() in word_happiness.keys()]\n",
    "    \n",
    "    # tweets which do not contain any words in the happiness list return return 0\n",
    "    if len(happiness_avg)>0:\n",
    "        return sum(happiness_avg)/len(happiness_avg)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    return happiness_avg.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Create two lists: one containing tweets by democratic members, and the other with the tweets of republican members.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "democratic_tweets = []\n",
    "republican_tweets = []\n",
    "\n",
    "for key in tw_names:\n",
    "    \n",
    "    wikiName = twitterHandle.loc[twitterHandle['tw_name'] == key]['WikiPageName'].values[0]\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        tweet_text = tweets[wikiName]\n",
    "        tweet_list = tweet_text.split('\\n')\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        print(wikiName)\n",
    "    \n",
    "    \n",
    "    party = twitterHandle.loc[twitterHandle['tw_name'] == key]['Party'].values[0]\n",
    "\n",
    "    if party == 'Democratic':\n",
    "        democratic_tweets += tweet_list\n",
    "    elif party == 'Republican':\n",
    "        republican_tweets += tweet_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets file seems to be missing few records. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Calculate the sentiment of each tweet and plot the distribution of sentiment for each of the two lists. Are there significant differences between the two? Which party post more positive tweets?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculating the sentiments of tweets made by democratic politicians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing the variable to store sentiments of democrats\n",
    "democratic_tweet_sentiments = []\n",
    "\n",
    "#cnt = 0\n",
    "democratic_tweet_sentiments = np.zeros(len(democratic_tweets))\n",
    "for i in range(len(democratic_tweets)):\n",
    "    #if cnt%1000==0:\n",
    "        #print(cnt)\n",
    "    token = democratic_tweets[i]\n",
    "    tweet_tokens = nltk.word_tokenize(token.lower())\n",
    "    democratic_tweet_sentiments[i] = calc_sentiment(tweet_tokens,word_happiness)\n",
    "    #cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculating the sentiments of tweets made by republican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing the variable to store sentiments of republican\n",
    "republican_tweet_sentiments = []\n",
    "\n",
    "#cnt = 0\n",
    "republican_tweet_sentiments = np.zeros(len(republican_tweets))\n",
    "for i in range(len(republican_tweets)):\n",
    "    #if cnt%1000==0:\n",
    "        #print(cnt)\n",
    "    token = republican_tweets[i]\n",
    "    tweet_tokens = nltk.word_tokenize(token.lower())\n",
    "    republican_tweet_sentiments[i] = calc_sentiment(tweet_tokens,word_happiness)\n",
    "    #cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filtering out the sentiments whose happiness score is 0 or less than zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_idx = np.argwhere(democratic_tweet_sentiments>0)\n",
    "filtered_dem_sent = democratic_tweet_sentiments[filtered_idx]\n",
    "filtered_dem_tweets = []\n",
    "\n",
    "for idx in filtered_idx:\n",
    "    \n",
    "    filtered_dem_tweets.append(democratic_tweets[int(idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_idx = np.argwhere(republican_tweet_sentiments>0)\n",
    "filtered_rep_sent = republican_tweet_sentiments[filtered_idx]\n",
    "filtered_rep_tweets = []\n",
    "\n",
    "for idx in filtered_idx:\n",
    "    \n",
    "    filtered_rep_tweets.append(republican_tweets[int(idx)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualizing the sentiments of both parties using histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histbins = np.linspace(min(min(filtered_rep_sent),min(filtered_dem_sent)),max(max(filtered_rep_sent),max(filtered_dem_sent)),30)\n",
    "\n",
    "histbins1 = [item for sublist in histbins for item in sublist]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(filtered_dem_sent,color='b',bins=histbins1,alpha=0.5)\n",
    "plt.hist(filtered_rep_sent,color='r',bins=histbins1,alpha=0.5)\n",
    "plt.xlabel('Sentiment score')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(['Democratic','Republican'])\n",
    "plt.xlim(3,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The democratic politicians seems to be tweeting slightly more positively compared to their political opponent the republican party. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Compute the average $m$ and standard deviation $\\sigma$ of the Tweets sentiment (considering tweets by both republican and democrats).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_sent = np.concatenate((filtered_dem_sent,filtered_rep_sent))\n",
    "overall_sent_mean = np.mean(overall_sent)\n",
    "overall_sent_std = np.std(overall_sent)\n",
    "\n",
    "print('The mean of all the sentiments of the tweets made by both parties is {}'.format(overall_sent_mean))\n",
    "print('The standard deviation of all the sentiments of the tweets made by both parties is {}'.format(overall_sent_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Defining a function to compute tf_idf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tf_idf(tokens,doc_token_sets):\n",
    "    # calculate tf-idf\n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    ndocs = len(doc_token_sets)\n",
    "    tf_idf_list = []\n",
    "    for item in fdist.most_common():\n",
    "        word = item[0]\n",
    "        cnt = 0\n",
    "        for i in range(ndocs):\n",
    "            if word in doc_token_sets[i]:\n",
    "                cnt += 1\n",
    "                \n",
    "        try:\n",
    "            \n",
    "            idf = math.log(ndocs/cnt)\n",
    "            tf = item[1]\n",
    "\n",
    "            tf_idf = tf*idf\n",
    "            tf_idf_list.append((word,tf_idf))\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print('Exception')\n",
    "            \n",
    "    tf_idf_list = sorted(tf_idf_list, key=lambda x: x[1],reverse = True)  \n",
    "    \n",
    "    return tf_idf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english') + [u'rt'] # add retweet tag to stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_tokens = []\n",
    "dem_doc_tokens = []\n",
    "rep_tokens = []\n",
    "rep_doc_tokens = []\n",
    "for key in tw_names:\n",
    "    \n",
    "    wikiName = twitterHandle.loc[twitterHandle['tw_name'] == key]['WikiPageName'].values[0]\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        text = tweets[wikiName]#.decode('unicode-escape')\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        # set to lower case\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        # check if word is alphabetic (removes numbers, hashtags, @ and punctuation) and remove stopwords\n",
    "        tokens = [t for t in tokens if t not in stopwords and t.isalpha()]\n",
    "        # remove twitter handles\n",
    "        tokens = [t for t in tokens if t not in tw_names]\n",
    "\n",
    "        party = twitterHandle.loc[twitterHandle['tw_name'] == key]['Party'].values[0]\n",
    "\n",
    "        if party == 'Democratic':\n",
    "            dem_tokens += tokens\n",
    "            dem_doc_tokens.append(set(tokens))\n",
    "        elif party == 'Republican':\n",
    "            rep_tokens += tokens\n",
    "            rep_doc_tokens.append(set(tokens))\n",
    "    \n",
    "    except:\n",
    "        print(wikiName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the twitter files for couple of politician is missing in the twitter dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Now only tweets with sentiment lower than $m-2\\sigma$. We will refer to them as negative tweets. Build a list containing negative tweets written by democrats, and one for republicans.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating a list of negative tweets made by democrats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtering out all the democratic tweets with sentiment ratings over m - 2 std\n",
    "neg_idx = np.argwhere(filtered_dem_sent<(overall_sent_mean-2*overall_sent_std))[:,0]\n",
    "neg_dem_tweets = []\n",
    "for idx in neg_idx:\n",
    "    neg_dem_tweets.append(filtered_dem_tweets[int(idx)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extracting all negative tweets made by democrats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_dem_tokens = []\n",
    "for tw in neg_dem_tweets:\n",
    "    tokens = nltk.word_tokenize(tw)\n",
    "    # set to lower case\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    # check if word is alphabetic (removes numbers, hashtags, @ and punctuation) and remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords and t.isalpha()]\n",
    "    # remove twitter handles\n",
    "    tokens = [t for t in tokens if t not in tw_names]\n",
    "    \n",
    "    neg_dem_tokens += tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Computing tf-idf for democratic negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_neg_dem = calc_tf_idf(neg_dem_tokens,dem_doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_dem_str = ''\n",
    "for item in tf_idf_neg_dem:\n",
    "    word = item[0]\n",
    "    count = int(item[1])\n",
    "    neg_dem_str += (word + ' ')*count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Negative democratic wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_neg_dem = WordCloud(background_color=\"white\",width=400, height=400,colormap=plt.cm.Blues,collocations=False)\n",
    "wc_neg_dem.generate(neg_dem_str)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wc_neg_dem,interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating a list of negative tweets made by republican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtering out all the republican tweets with sentiment ratings over m - 2 std\n",
    "neg_idx = np.argwhere(filtered_rep_sent<(overall_sent_mean-2*overall_sent_std))[:,0]\n",
    "neg_rep_tweets = []\n",
    "for idx in neg_idx:\n",
    "    neg_rep_tweets.append(filtered_rep_tweets[int(idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_rep_tokens = []\n",
    "for tw in neg_rep_tweets:\n",
    "    tokens = nltk.word_tokenize(tw)\n",
    "    # set to lower case\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    # check if word is alphabetic (removes numbers, hashtags, @ and punctuation) and remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords and t.isalpha()]\n",
    "    # remove twitter handles\n",
    "    tokens = [t for t in tokens if t not in tw_names]\n",
    "    \n",
    "    neg_rep_tokens += tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Computing tf-idf for republican negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_neg_rep = calc_tf_idf(neg_rep_tokens,rep_doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_rep_str = ''\n",
    "for item in tf_idf_neg_rep:\n",
    "    word = item[0]\n",
    "    count = int(item[1])\n",
    "    neg_rep_str += (word + ' ')*count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Negative republican wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_neg_rep = WordCloud(background_color=\"white\",width=400, height=400,colormap=plt.cm.Reds,collocations=False)\n",
    "wc_neg_rep.generate(neg_rep_str)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wc_neg_rep,interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion of negative sentiments:**  \n",
    "\n",
    "The democrats seems to be focusing on topics such as violence, death and imprisonment. However, the republicans seems to be focusing on other topics such as weather, economy, guilt and failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Repeat the point above, but considering positive tweets (e.g. with sentiment larger than $m+2\\sigma$). Comment on your results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating a list of positive tweets made by democrats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtering out all the tweets of democrats with sentimentaL score less than m + 2 std\n",
    "pos_idx = np.argwhere(filtered_dem_sent>(overall_sent_mean+2*overall_sent_std))[:,0]\n",
    "pos_dem_tweets = []\n",
    "for idx in pos_idx:\n",
    "    pos_dem_tweets.append(filtered_dem_tweets[int(idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dem_tokens = []\n",
    "for tw in pos_dem_tweets:\n",
    "    tokens = nltk.word_tokenize(tw)\n",
    "    # set to lower case\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    # check if word is alphabetic (removes numbers, hashtags, @ and punctuation) and remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords and t.isalpha()]\n",
    "    # remove twitter handles\n",
    "    tokens = [t for t in tokens if t not in tw_names]\n",
    "    \n",
    "    pos_dem_tokens += tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Computing tf-idf for democrats positive sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_pos_dem = calc_tf_idf(pos_dem_tokens,dem_doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dem_str = ''\n",
    "for item in tf_idf_pos_dem:\n",
    "    word = item[0]\n",
    "    count = int(item[1])\n",
    "    pos_dem_str += (word + ' ')*count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Positive democratic wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_pos_dem = WordCloud(background_color=\"white\",width=400, height=400,colormap=plt.cm.Blues,collocations=False)\n",
    "wc_pos_dem.generate(pos_dem_str)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wc_pos_dem,interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating a list of positive tweets made by republican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_idx = np.argwhere(filtered_rep_sent>(overall_sent_mean+2*overall_sent_std))[:,0]\n",
    "pos_rep_tweets = []\n",
    "for idx in pos_idx:\n",
    "    pos_rep_tweets.append(filtered_rep_tweets[int(idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rep_tokens = []\n",
    "for tw in pos_rep_tweets:\n",
    "    tokens = nltk.word_tokenize(tw)\n",
    "    # set to lower case\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    # check if word is alphabetic (removes numbers, hashtags, @ and punctuation) and remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords and t.isalpha()]\n",
    "    # remove twitter handles\n",
    "    tokens = [t for t in tokens if t not in tw_names]\n",
    "    \n",
    "    pos_rep_tokens += tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Computing tf-idf for republican positive sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_pos_rep = calc_tf_idf(pos_rep_tokens,rep_doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rep_str = ''\n",
    "for item in tf_idf_pos_rep:\n",
    "    word = item[0]\n",
    "    count = int(item[1])\n",
    "    pos_rep_str += (word + ' ')*count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Positive republican wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_pos_rep = WordCloud(background_color=\"white\",width=400, height=400,colormap=plt.cm.Reds,collocations=False)\n",
    "wc_pos_rep.generate(pos_rep_str)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wc_pos_rep,interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion of Positive sentiments:**  \n",
    "\n",
    "Both political parties seems to be focusing on greetings and annual holidays and independence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
